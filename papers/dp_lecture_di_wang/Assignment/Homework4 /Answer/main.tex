\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{datetime}
\usepackage{amssymb}
\usepackage{hyperref}

\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\newdate{date}{21}{11}{2021}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\date{\displaydate{date}}
\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\begin{document}

% ========== Edit your name here
\author{Lijie Hu}
\title{Homework 4}
\date{}
\maketitle
\section{Problem 1}
We first prove it is $\epsilon$-LDP, by the definition of $Y_i$ we known that $\text{Pr}(Y_i=1), \text{Pr}(Y_i=0) \in [\frac{1}{e^\epsilon+1}, \frac{e^\epsilon}{e^\epsilon+1}$
no matter what $x_i$ is. Thus we always have $e^{-\epsilon}\leq \frac{\text{Pr}(Y_i=1|x_i)}{\text{Pr}(Y_i=1|x_i')}\leq e^\epsilon$ for any pair of $x_i, x_i'$. 

We then consider the unbiaseness, we have 
\begin{align*}
\mathbb{E}(\hat{\mu}) &= \frac{m}{n}(\sum_{i=1}^n \mathbb{E}[Y_i]\frac{e^\epsilon+1}{e^\epsilon-1}-	\frac{1}{e^\epsilon-1})\\
&=\frac{m}{n}(\sum_{i=1}^n (\frac{1}{e^\epsilon+1}+\frac{x_i (e^\epsilon-1)}{m (e^\epsilon+1})) \frac{e^\epsilon+1}{e^\epsilon-1}-	\frac{1}{e^\epsilon-1})\\
&=\frac{1}{n}\sum_{i=1}^n x_i. 
\end{align*}
Finally we will show the variance, first we have 
\begin{equation}
	\hat{\mu}-\mu= \frac{m}{n}\sum_{i=1}^n(Y_i-\mathbb{E}(Y_i))\frac{e^\epsilon+1}{e^\epsilon-1}.
\end{equation}
Thus, 
\begin{equation}
	\text{Var}(\hat{\mu})=\frac{m^2}{n^2}(\frac{e^\epsilon+1}{e^\epsilon-1})^2\sum_{i=1}^n \text{Var}(Y_i)=O(\frac{m^2}{n\epsilon^2} \text{Var}(Y_i)).
\end{equation}
For each $i$ since $Y_i\in [0, 1]$ thus we have
\begin{align*}
	\text{Var}(Y_i)\leq 1
	\end{align*}.
	Thus we have the proof. 
	\section{Problem 2}
	We first proof it is $\epsilon$-LDP.  By the definition of $z_i$ we known that $\text{Pr}(z_i=z)\in [\frac{1}{e^\epsilon+1}, \frac{e^\epsilon}{e^\epsilon+1}]$ for any possible output, 
no matter what $x_i$ is. Thus we always have $e^{-\epsilon}\leq \frac{\text{Pr}(Y_i=1|x_i)}{\text{Pr}(Y_i=1|x_i')}\leq e^\epsilon$ for any pair of $x_i, x_i'$. 

For unbiaseness, if $x=0$, then we can see that $\mathbb{E}[z]=0$. Otherwise by definition 
\begin{align*}
	\mathbb{E}[z_i]=\frac{1}{m}\sum_{j=1}^m (0, 0, \cdots, \mathbb{E}[z_{i,j}],0, \cdots, 0)= \frac{1}{m}\sum_{j=1}^m (0, 0, \cdots, mx_{i, j},0, \cdots, 0)=x_i. 
\end{align*}
\section{Problem 3} 
I mainly follow the idea in [2]. Consider a fixed  distance $k$, the worst case is that  we can change up to $k$ entries in $x_1, ..., x_n$ to $0$ or $\Lambda$ and then
change the median value from $x_{m-k}$ to $x_{m+k}$. Therefore, when the median is an end
point of a large empty interval the local sensitivity at distance k is
maximized. In order to achieve that, we can modify entries $x_{m-k+1}, \cdots, x_{m-1+t}$ for some $t=0, \cdots, k+1$ to $0$ or  $\Lambda$. By the definition of the smooth sensitivity we have 
\begin{align*}
A^{(k)}(x)=\max_{y: d(x, y)\leq k}LS(y)=\max_{0\leq t\leq k+1}(x_{m+t}-x_{m+t-k-1}) 	
\end{align*}
And, then we have: 
\begin{equation}
	S_{f, \epsilon)(D)}=\max_{k=0, \cdots, n}(e^{-k\epsilon} \max_{0\leq t\leq k+1}(x_{m+t}-x_{m+t-k-1}). 
\end{equation}
\section{Problem 4}
To prove the algorithm is $(\epsilon, \delta)$-DP, we know that using the AboveThreshold for finding one query that is larger than $c$ is $\epsilon'$-DP with $\epsilon'=\frac{\epsilon}{2\sqrt{2c\log 2/\delta}}$. Thus, by the composition theorem (Corollary 7.4) we can see that the algorithm is $(\epsilon, \delta)$-DP. The same for the $\epsilon$-DP. 

Next we focus on the accuracy, first by Theorem 9.2 we know that Abovethreshold is $(\alpha, \beta)$-accurate for $\alpha=\frac{8(\log k+\log 2/\beta)}{\epsilon}$. Since Algorithm 2 is $c$ compositions of Abovethreshold. Thus here $\epsilon=\epsilon'=\frac{\epsilon}{2\sqrt{2c\log 2/\delta}}$ and $\beta=\frac{\beta}{c}$ since we need the accuracy holds for each query. Thus, for $(\epsilon, \delta)$-DP, Algorithm 2 will be $(\alpha, \beta)$-accurate for $\alpha=\frac{16\sqrt{2c\log 2/\delta}(\log k+\log 2c/\beta )}{\epsilon}$. Thus same for $\epsilon$-DP. 
\end{document}
\grid
\grid